# importing the libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.metrics import classification_report,accuracy_score
from pylab import rcParams
rcParams['figure.figsize'] = 14, 8
RANDOM_SEED = 42
LABELS = ["Normal", "Fraud"]

# loading the dataset
dataset=pd.read_csv("C:/Users/Harshit/Downloads/creditcard.csv/creditcard.csv",delimiter = ',')
dataset
dataset.info()

# Explotary data analysis
dataset.isnull().values.any()
count_classes = pd.value_counts(dataset['Class'],sort = True)
count_classes.plot(kind = 'bar',rot=0)
plt.title("Transaction classes Distrubition")
plt.xticks(range(2), LABELS)
plt.xlabel("Class")
plt.ylabel("Frequency")

# get the fraud and the normal dataset
# here 1 denots for fraud transition
fraud = dataset[dataset['Class']==1] 
#here 0 denots for the normal transition
Normal = dataset[dataset['Class']==0] 

# printing the normal and the fraud transition in the dataset
print(fraud.shape,Normal.shape)  

# now here we need to analyse more amount of the information from the transition data 
# How different are the amoount of money used in the different transition classes
fraud.Amount.describe()
Normal.Amount.describe()

#ploting the graph of fraud transaction and the normal transaction wrt amount
f,(ax1,ax2)=plt.subplots(2,1,sharex=True)
f.suptitle('amount per transition by the class')
bins=50
ax1.hist(fraud.Amount , bins = bins)
ax1.set_title('Fraud')
ax2.hist(Normal.Amount, bins = bins)
ax2.set_title('Normal')
plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.xlim((0, 20000))
plt.yscale('log')
plt.show();

# Here we are checking the fraudulent transactions occur more often during certain time frame
#like how many different transaction are there for fraud and the normal transaction

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)
f.suptitle('Time of transaction vs Amount by class')
ax1.scatter(fraud.Time, fraud.Amount)
ax1.set_title('Fraud')
ax2.scatter(Normal.Time, Normal.Amount)
ax2.set_title('Normal')
plt.xlabel('Time (in Seconds)')
plt.ylabel('Amount')
plt.show()

# Take some sample of the data
#here i am takeing 1% of whole dataset data
dataset1= dataset.sample(frac = 0.1,random_state=1)

dataset1.shape
dataset.shape

#Determine the number of fraud and valid transactions in the dataset

fraud = dataset1[dataset1['Class']==1]
Valid = dataset1[dataset1['Class']==0]
outlier_fraction = len(fraud)/float(len(Valid))

# Correlation in dataset by ploting the heat map
#getting the corelation of each features in dataset
corrmat = dataset1.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
g=sns.heatmap(dataset[top_corr_features].corr(),annot=True,cmap="RdYlGn")

# now here i am creating the independent and the dependent features
columns = dataset1.columns.tolist()
# Filter the columns to remove data we do not want 
columns = [c for c in columns if c not in ["Class"]]
# Store the variable we are predicting 
target = "Class"
# Define a random state 
state = np.random.RandomState(42)
X = dataset1[columns] #independent variable
Y = dataset1[target] #dependent variable
X_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))
# Print the shapes of X & Y
print(X.shape)
print(Y.shape)

# NOW LETS DO THE THE MODEL PREDICTION
# ISOLATION FOREST ALGORITHEM
#LOCAL OUTLIER FACTOR(LOF) ALGORITHEM
#SUPPORT VECTOR MACHINE (SVM)

#Define the outlier detection methods

classifiers = {
    "Isolation Forest":IsolationForest(n_estimators=100, max_samples=len(X), 
                                       contamination=outlier_fraction,random_state=state, verbose=0),
    "Local Outlier Factor":LocalOutlierFactor(n_neighbors=20, algorithm='auto', 
                                              leaf_size=30, metric='minkowski',
                                              p=2, metric_params=None, contamination=outlier_fraction),
    "Support Vector Machine":OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, 
                                         max_iter=-1)
}

type(classifiers)

n_outliers = len(fraud)
for i, (clf_name,clf) in enumerate(classifiers.items()):
    #Fit the data and tag outliers
    if clf_name == "Local Outlier Factor":
        y_pred = clf.fit_predict(X)
        scores_prediction = clf.negative_outlier_factor_
    elif clf_name == "Support Vector Machine":
        clf.fit(X)
        y_pred = clf.predict(X)
    else:    
        clf.fit(X)
        scores_prediction = clf.decision_function(X)
        y_pred = clf.predict(X)
    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions
    y_pred[y_pred == 1] = 0
    y_pred[y_pred == -1] = 1
    n_errors = (y_pred != Y).sum()
    # Run Classification Metrics
    print("{}: {}".format(clf_name,n_errors))
    print("Accuracy Score :")
    print(accuracy_score(Y,y_pred))
    print("Classification Report :")
    print(classification_report(Y,y_pred))
















